{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomli\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.toml\", \"rb\") as f:\n",
    "    config = tomli.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>gender</th>\n",
       "      <th>hold racket handed</th>\n",
       "      <th>play years</th>\n",
       "      <th>level</th>\n",
       "      <th>cut_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0   61  122  183  244  305  366  428  489 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0   74  149  224  299  374  449  524  599 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0  103  207  311  415  519  623  727  831 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0  101  203  304  406  507  609  710  812 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0  105  211  317  423  529  635  740  846 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  player_id  mode  gender  hold racket handed  play years  level  \\\n",
       "0          1         41     1       1                   1           1      5   \n",
       "1          2         41     2       1                   1           1      5   \n",
       "2          3         41     3       1                   1           1      5   \n",
       "3          4         41     4       1                   1           1      5   \n",
       "4          5         41     5       1                   1           1      5   \n",
       "\n",
       "                                           cut_point  \n",
       "0  [   0   61  122  183  244  305  366  428  489 ...  \n",
       "1  [   0   74  149  224  299  374  449  524  599 ...  \n",
       "2  [   0  103  207  311  415  519  623  727  831 ...  \n",
       "3  [   0  101  203  304  406  507  609  710  812 ...  \n",
       "4  [   0  105  211  317  423  529  635  740  846 ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info = pd.read_csv(config['Train']['InfoFile'])\n",
    "train_info.head()\n",
    "# train_info['level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_players = train_info['player_id'].unique()\n",
    "train_players, test_players = train_test_split(unique_players, test_size=0.2, random_state=42)\n",
    "\n",
    "# 讀取特徵 CSV 檔（位於 \"./tabular_data_train\"）\n",
    "datapath = './tabular_data_train'\n",
    "datalist = glob.glob(os.path.join(config['Train']['FeaturePath'], \"*.csv\"))\n",
    "target_mask = ['gender', 'hold racket handed', 'play years', 'level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax_mean</th>\n",
       "      <th>ay_mean</th>\n",
       "      <th>az_mean</th>\n",
       "      <th>gx_mean</th>\n",
       "      <th>gy_mean</th>\n",
       "      <th>gz_mean</th>\n",
       "      <th>ax_var</th>\n",
       "      <th>ay_var</th>\n",
       "      <th>az_var</th>\n",
       "      <th>gx_var</th>\n",
       "      <th>...</th>\n",
       "      <th>a_fft</th>\n",
       "      <th>g_fft</th>\n",
       "      <th>a_psd</th>\n",
       "      <th>g_psd</th>\n",
       "      <th>a_kurt</th>\n",
       "      <th>g_kurt</th>\n",
       "      <th>a_skewn</th>\n",
       "      <th>g_skewn</th>\n",
       "      <th>a_entropy</th>\n",
       "      <th>g_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3210.325581</td>\n",
       "      <td>-1472.639535</td>\n",
       "      <td>769.174419</td>\n",
       "      <td>3888.139535</td>\n",
       "      <td>15000.069767</td>\n",
       "      <td>-1508.011628</td>\n",
       "      <td>2625.734377</td>\n",
       "      <td>2141.530712</td>\n",
       "      <td>2579.672441</td>\n",
       "      <td>22376.099388</td>\n",
       "      <td>...</td>\n",
       "      <td>34235.187587</td>\n",
       "      <td>34235.187587</td>\n",
       "      <td>8.524912e+09</td>\n",
       "      <td>8.524912e+09</td>\n",
       "      <td>7.929079</td>\n",
       "      <td>2.715360</td>\n",
       "      <td>193.153135</td>\n",
       "      <td>63.072307</td>\n",
       "      <td>-0.055732</td>\n",
       "      <td>-0.055732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4266.151163</td>\n",
       "      <td>-1758.790698</td>\n",
       "      <td>433.104651</td>\n",
       "      <td>4868.604651</td>\n",
       "      <td>14022.290698</td>\n",
       "      <td>3694.767442</td>\n",
       "      <td>2716.650412</td>\n",
       "      <td>3104.921440</td>\n",
       "      <td>2033.799252</td>\n",
       "      <td>23827.776754</td>\n",
       "      <td>...</td>\n",
       "      <td>4200.628815</td>\n",
       "      <td>4200.628815</td>\n",
       "      <td>2.855087e+08</td>\n",
       "      <td>2.855087e+08</td>\n",
       "      <td>4.805886</td>\n",
       "      <td>3.387055</td>\n",
       "      <td>116.344046</td>\n",
       "      <td>86.140297</td>\n",
       "      <td>-0.055453</td>\n",
       "      <td>-0.055453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3777.058140</td>\n",
       "      <td>-2363.441860</td>\n",
       "      <td>496.430233</td>\n",
       "      <td>3838.127907</td>\n",
       "      <td>17846.848837</td>\n",
       "      <td>2016.662791</td>\n",
       "      <td>2646.756001</td>\n",
       "      <td>2342.910897</td>\n",
       "      <td>2150.277547</td>\n",
       "      <td>25096.946277</td>\n",
       "      <td>...</td>\n",
       "      <td>8077.753674</td>\n",
       "      <td>8077.753674</td>\n",
       "      <td>2.063152e+08</td>\n",
       "      <td>2.063152e+08</td>\n",
       "      <td>3.828190</td>\n",
       "      <td>2.898664</td>\n",
       "      <td>113.018318</td>\n",
       "      <td>54.248883</td>\n",
       "      <td>-0.056366</td>\n",
       "      <td>-0.056366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3545.058140</td>\n",
       "      <td>-1156.709302</td>\n",
       "      <td>618.046512</td>\n",
       "      <td>3811.674419</td>\n",
       "      <td>14340.662791</td>\n",
       "      <td>3557.930233</td>\n",
       "      <td>2507.900301</td>\n",
       "      <td>2258.403467</td>\n",
       "      <td>1908.365970</td>\n",
       "      <td>22559.762748</td>\n",
       "      <td>...</td>\n",
       "      <td>5265.867665</td>\n",
       "      <td>5265.867665</td>\n",
       "      <td>1.557855e+08</td>\n",
       "      <td>1.557855e+08</td>\n",
       "      <td>2.039432</td>\n",
       "      <td>3.110605</td>\n",
       "      <td>30.070979</td>\n",
       "      <td>67.902479</td>\n",
       "      <td>-0.056986</td>\n",
       "      <td>-0.056986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3078.068966</td>\n",
       "      <td>-793.758621</td>\n",
       "      <td>137.551724</td>\n",
       "      <td>10518.264368</td>\n",
       "      <td>9810.137931</td>\n",
       "      <td>-9929.160920</td>\n",
       "      <td>2518.215706</td>\n",
       "      <td>2126.238452</td>\n",
       "      <td>1811.472957</td>\n",
       "      <td>19047.955661</td>\n",
       "      <td>...</td>\n",
       "      <td>1315.209915</td>\n",
       "      <td>1315.209915</td>\n",
       "      <td>6.176466e+07</td>\n",
       "      <td>6.176466e+07</td>\n",
       "      <td>8.145404</td>\n",
       "      <td>5.094156</td>\n",
       "      <td>205.116213</td>\n",
       "      <td>137.770788</td>\n",
       "      <td>-0.057376</td>\n",
       "      <td>-0.057376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ax_mean      ay_mean     az_mean       gx_mean       gy_mean  \\\n",
       "0 -3210.325581 -1472.639535  769.174419   3888.139535  15000.069767   \n",
       "1 -4266.151163 -1758.790698  433.104651   4868.604651  14022.290698   \n",
       "2 -3777.058140 -2363.441860  496.430233   3838.127907  17846.848837   \n",
       "3 -3545.058140 -1156.709302  618.046512   3811.674419  14340.662791   \n",
       "4 -3078.068966  -793.758621  137.551724  10518.264368   9810.137931   \n",
       "\n",
       "       gz_mean       ax_var       ay_var       az_var        gx_var  ...  \\\n",
       "0 -1508.011628  2625.734377  2141.530712  2579.672441  22376.099388  ...   \n",
       "1  3694.767442  2716.650412  3104.921440  2033.799252  23827.776754  ...   \n",
       "2  2016.662791  2646.756001  2342.910897  2150.277547  25096.946277  ...   \n",
       "3  3557.930233  2507.900301  2258.403467  1908.365970  22559.762748  ...   \n",
       "4 -9929.160920  2518.215706  2126.238452  1811.472957  19047.955661  ...   \n",
       "\n",
       "          a_fft         g_fft         a_psd         g_psd    a_kurt    g_kurt  \\\n",
       "0  34235.187587  34235.187587  8.524912e+09  8.524912e+09  7.929079  2.715360   \n",
       "1   4200.628815   4200.628815  2.855087e+08  2.855087e+08  4.805886  3.387055   \n",
       "2   8077.753674   8077.753674  2.063152e+08  2.063152e+08  3.828190  2.898664   \n",
       "3   5265.867665   5265.867665  1.557855e+08  1.557855e+08  2.039432  3.110605   \n",
       "4   1315.209915   1315.209915  6.176466e+07  6.176466e+07  8.145404  5.094156   \n",
       "\n",
       "      a_skewn     g_skewn  a_entropy  g_entropy  \n",
       "0  193.153135   63.072307  -0.055732  -0.055732  \n",
       "1  116.344046   86.140297  -0.055453  -0.055453  \n",
       "2  113.018318   54.248883  -0.056366  -0.056366  \n",
       "3   30.070979   67.902479  -0.056986  -0.056986  \n",
       "4  205.116213  137.770788  -0.057376  -0.057376  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file = pd.read_csv(datalist[0])\n",
    "sample_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據 test_players 分組資料\n",
    "x_train = pd.DataFrame()\n",
    "y_train = pd.DataFrame(columns=target_mask)\n",
    "x_test = pd.DataFrame()\n",
    "y_test = pd.DataFrame(columns=target_mask)\n",
    "\n",
    "for file in datalist:\n",
    "    unique_id = int(Path(file).stem)\n",
    "    row = train_info[train_info['unique_id'] == unique_id]\n",
    "    if row.empty:\n",
    "        continue\n",
    "    player_id = row['player_id'].iloc[0]\n",
    "    data = pd.read_csv(file)\n",
    "    target = row[target_mask]\n",
    "    target_repeated = pd.concat([target] * len(data))\n",
    "    if player_id in train_players:\n",
    "        x_train = pd.concat([x_train, data], ignore_index=True)\n",
    "        y_train = pd.concat([y_train, target_repeated], ignore_index=True)\n",
    "    elif player_id in test_players:\n",
    "        x_test = pd.concat([x_test, data], ignore_index=True)\n",
    "        y_test = pd.concat([y_test, target_repeated], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化特徵\n",
    "scaler = MinMaxScaler()\n",
    "le = LabelEncoder()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "group_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax_mean</th>\n",
       "      <th>ay_mean</th>\n",
       "      <th>az_mean</th>\n",
       "      <th>gx_mean</th>\n",
       "      <th>gy_mean</th>\n",
       "      <th>gz_mean</th>\n",
       "      <th>ax_var</th>\n",
       "      <th>ay_var</th>\n",
       "      <th>az_var</th>\n",
       "      <th>gx_var</th>\n",
       "      <th>...</th>\n",
       "      <th>a_fft</th>\n",
       "      <th>g_fft</th>\n",
       "      <th>a_psd</th>\n",
       "      <th>g_psd</th>\n",
       "      <th>a_kurt</th>\n",
       "      <th>g_kurt</th>\n",
       "      <th>a_skewn</th>\n",
       "      <th>g_skewn</th>\n",
       "      <th>a_entropy</th>\n",
       "      <th>g_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3210.325581</td>\n",
       "      <td>-1472.639535</td>\n",
       "      <td>769.174419</td>\n",
       "      <td>3888.139535</td>\n",
       "      <td>15000.069767</td>\n",
       "      <td>-1508.011628</td>\n",
       "      <td>2625.734377</td>\n",
       "      <td>2141.530712</td>\n",
       "      <td>2579.672441</td>\n",
       "      <td>22376.099388</td>\n",
       "      <td>...</td>\n",
       "      <td>34235.187587</td>\n",
       "      <td>34235.187587</td>\n",
       "      <td>8.524912e+09</td>\n",
       "      <td>8.524912e+09</td>\n",
       "      <td>7.929079</td>\n",
       "      <td>2.715360</td>\n",
       "      <td>193.153135</td>\n",
       "      <td>63.072307</td>\n",
       "      <td>-0.055732</td>\n",
       "      <td>-0.055732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4266.151163</td>\n",
       "      <td>-1758.790698</td>\n",
       "      <td>433.104651</td>\n",
       "      <td>4868.604651</td>\n",
       "      <td>14022.290698</td>\n",
       "      <td>3694.767442</td>\n",
       "      <td>2716.650412</td>\n",
       "      <td>3104.921440</td>\n",
       "      <td>2033.799252</td>\n",
       "      <td>23827.776754</td>\n",
       "      <td>...</td>\n",
       "      <td>4200.628815</td>\n",
       "      <td>4200.628815</td>\n",
       "      <td>2.855087e+08</td>\n",
       "      <td>2.855087e+08</td>\n",
       "      <td>4.805886</td>\n",
       "      <td>3.387055</td>\n",
       "      <td>116.344046</td>\n",
       "      <td>86.140297</td>\n",
       "      <td>-0.055453</td>\n",
       "      <td>-0.055453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3777.058140</td>\n",
       "      <td>-2363.441860</td>\n",
       "      <td>496.430233</td>\n",
       "      <td>3838.127907</td>\n",
       "      <td>17846.848837</td>\n",
       "      <td>2016.662791</td>\n",
       "      <td>2646.756001</td>\n",
       "      <td>2342.910897</td>\n",
       "      <td>2150.277547</td>\n",
       "      <td>25096.946277</td>\n",
       "      <td>...</td>\n",
       "      <td>8077.753674</td>\n",
       "      <td>8077.753674</td>\n",
       "      <td>2.063152e+08</td>\n",
       "      <td>2.063152e+08</td>\n",
       "      <td>3.828190</td>\n",
       "      <td>2.898664</td>\n",
       "      <td>113.018318</td>\n",
       "      <td>54.248883</td>\n",
       "      <td>-0.056366</td>\n",
       "      <td>-0.056366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3545.058140</td>\n",
       "      <td>-1156.709302</td>\n",
       "      <td>618.046512</td>\n",
       "      <td>3811.674419</td>\n",
       "      <td>14340.662791</td>\n",
       "      <td>3557.930233</td>\n",
       "      <td>2507.900301</td>\n",
       "      <td>2258.403467</td>\n",
       "      <td>1908.365970</td>\n",
       "      <td>22559.762748</td>\n",
       "      <td>...</td>\n",
       "      <td>5265.867665</td>\n",
       "      <td>5265.867665</td>\n",
       "      <td>1.557855e+08</td>\n",
       "      <td>1.557855e+08</td>\n",
       "      <td>2.039432</td>\n",
       "      <td>3.110605</td>\n",
       "      <td>30.070979</td>\n",
       "      <td>67.902479</td>\n",
       "      <td>-0.056986</td>\n",
       "      <td>-0.056986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3078.068966</td>\n",
       "      <td>-793.758621</td>\n",
       "      <td>137.551724</td>\n",
       "      <td>10518.264368</td>\n",
       "      <td>9810.137931</td>\n",
       "      <td>-9929.160920</td>\n",
       "      <td>2518.215706</td>\n",
       "      <td>2126.238452</td>\n",
       "      <td>1811.472957</td>\n",
       "      <td>19047.955661</td>\n",
       "      <td>...</td>\n",
       "      <td>1315.209915</td>\n",
       "      <td>1315.209915</td>\n",
       "      <td>6.176466e+07</td>\n",
       "      <td>6.176466e+07</td>\n",
       "      <td>8.145404</td>\n",
       "      <td>5.094156</td>\n",
       "      <td>205.116213</td>\n",
       "      <td>137.770788</td>\n",
       "      <td>-0.057376</td>\n",
       "      <td>-0.057376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ax_mean      ay_mean     az_mean       gx_mean       gy_mean  \\\n",
       "0 -3210.325581 -1472.639535  769.174419   3888.139535  15000.069767   \n",
       "1 -4266.151163 -1758.790698  433.104651   4868.604651  14022.290698   \n",
       "2 -3777.058140 -2363.441860  496.430233   3838.127907  17846.848837   \n",
       "3 -3545.058140 -1156.709302  618.046512   3811.674419  14340.662791   \n",
       "4 -3078.068966  -793.758621  137.551724  10518.264368   9810.137931   \n",
       "\n",
       "       gz_mean       ax_var       ay_var       az_var        gx_var  ...  \\\n",
       "0 -1508.011628  2625.734377  2141.530712  2579.672441  22376.099388  ...   \n",
       "1  3694.767442  2716.650412  3104.921440  2033.799252  23827.776754  ...   \n",
       "2  2016.662791  2646.756001  2342.910897  2150.277547  25096.946277  ...   \n",
       "3  3557.930233  2507.900301  2258.403467  1908.365970  22559.762748  ...   \n",
       "4 -9929.160920  2518.215706  2126.238452  1811.472957  19047.955661  ...   \n",
       "\n",
       "          a_fft         g_fft         a_psd         g_psd    a_kurt    g_kurt  \\\n",
       "0  34235.187587  34235.187587  8.524912e+09  8.524912e+09  7.929079  2.715360   \n",
       "1   4200.628815   4200.628815  2.855087e+08  2.855087e+08  4.805886  3.387055   \n",
       "2   8077.753674   8077.753674  2.063152e+08  2.063152e+08  3.828190  2.898664   \n",
       "3   5265.867665   5265.867665  1.557855e+08  1.557855e+08  2.039432  3.110605   \n",
       "4   1315.209915   1315.209915  6.176466e+07  6.176466e+07  8.145404  5.094156   \n",
       "\n",
       "      a_skewn     g_skewn  a_entropy  g_entropy  \n",
       "0  193.153135   63.072307  -0.055732  -0.055732  \n",
       "1  116.344046   86.140297  -0.055453  -0.055453  \n",
       "2  113.018318   54.248883  -0.056366  -0.056366  \n",
       "3   30.070979   67.902479  -0.056986  -0.056986  \n",
       "4  205.116213  137.770788  -0.057376  -0.057376  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple neural network for binary classification\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define a neural network for multi-class classification\n",
    "class MultiClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MultiClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        logits = self.model(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "# Function to train binary classification model with early stopping (using validation loss)\n",
    "def train_binary_model(X_train, y_train, X_val, y_val, batch_size=64, epochs=20, lr=0.001, patience=5, min_delta=1e-4,weights=None):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1))\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = BinaryClassifier(input_size)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    if weights != None:\n",
    "        torch_weight = torch.FloatTensor(weights)\n",
    "        criterion = nn.BCELoss(weight=torch_weight)\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop with early stopping (monitoring validation loss)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Compute validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check (on validation loss)\n",
    "        if best_val_loss - val_loss > min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(X_val_tensor).numpy().flatten()\n",
    "    \n",
    "    # Group predictions\n",
    "    num_groups = len(predicted) // group_size\n",
    "    y_pred = []\n",
    "    for i in range(num_groups):\n",
    "        group_preds = predicted[i*group_size: (i+1)*group_size]\n",
    "        if sum(group_preds[:group_size]) / group_size > 0.5:\n",
    "            y_pred.append(max(group_preds))\n",
    "        else:\n",
    "            y_pred.append(min(group_preds))\n",
    "    \n",
    "    y_val_agg = [y_val[i*group_size] for i in range(num_groups)]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_val_agg, y_pred, average='micro')\n",
    "    print(f'Binary AUC: {auc_score}')\n",
    "    return model, auc_score\n",
    "\n",
    "# Function to train multi-class classification model with early stopping (using validation loss)\n",
    "def train_multi_model(X_train, y_train, X_val, y_val, batch_size=64, epochs=20, lr=0.001, patience=5, min_delta=1e-4,weights=None):\n",
    "    # Get number of classes\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = MultiClassifier(input_size, num_classes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    if weights != None:\n",
    "        torch_weight = torch.FloatTensor(weights)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch_weight)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop with early stopping (monitoring validation loss)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Compute validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check (on validation loss)\n",
    "        if best_val_loss - val_loss > min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_val_tensor)\n",
    "        predicted = torch.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Group predictions as in your original code\n",
    "    num_groups = len(predicted) // group_size\n",
    "    y_pred = []\n",
    "    for i in range(num_groups):\n",
    "        group_pred = predicted[i*group_size: (i+1)*group_size]\n",
    "        class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n",
    "        chosen_class = np.argmax(class_sums)\n",
    "        candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n",
    "        best_instance = np.argmax(candidate_probs)\n",
    "        y_pred.append(group_pred[best_instance])\n",
    "    \n",
    "    y_val_agg = [y_val[i*group_size] for i in range(num_groups)]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_val_agg, y_pred, average='micro', multi_class='ovr')\n",
    "    print(f'Multi-class AUC: {auc_score}')\n",
    "    return model, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 2.2367, Val Loss: 1.6879\n",
      "Epoch 2/20, Train Loss: 1.9073, Val Loss: 1.6052\n",
      "Epoch 3/20, Train Loss: 1.6557, Val Loss: 1.5260\n",
      "Epoch 4/20, Train Loss: 1.4364, Val Loss: 1.5678\n",
      "Epoch 5/20, Train Loss: 1.3063, Val Loss: 2.0175\n",
      "Epoch 6/20, Train Loss: 1.1961, Val Loss: 2.0718\n",
      "Epoch 7/20, Train Loss: 1.1237, Val Loss: 1.5987\n",
      "Epoch 8/20, Train Loss: 1.0725, Val Loss: 2.2067\n",
      "Early stopping at epoch 8\n",
      "Binary AUC: 0.8620390653523183\n",
      "Epoch 1/20, Train Loss: 0.8103, Val Loss: 0.1888\n",
      "Epoch 2/20, Train Loss: 0.1779, Val Loss: 0.1496\n",
      "Epoch 3/20, Train Loss: 0.1251, Val Loss: 0.0979\n",
      "Epoch 4/20, Train Loss: 0.1019, Val Loss: 0.0632\n",
      "Epoch 5/20, Train Loss: 0.0889, Val Loss: 0.0534\n",
      "Epoch 6/20, Train Loss: 0.0796, Val Loss: 0.0803\n",
      "Epoch 7/20, Train Loss: 0.0762, Val Loss: 0.0630\n",
      "Epoch 8/20, Train Loss: 0.0714, Val Loss: 0.0443\n",
      "Epoch 9/20, Train Loss: 0.0627, Val Loss: 0.0660\n",
      "Epoch 10/20, Train Loss: 0.0627, Val Loss: 0.0811\n",
      "Epoch 11/20, Train Loss: 0.0685, Val Loss: 0.0496\n",
      "Epoch 12/20, Train Loss: 0.0617, Val Loss: 0.0630\n",
      "Epoch 13/20, Train Loss: 0.0586, Val Loss: 0.0727\n",
      "Early stopping at epoch 13\n",
      "Binary AUC: 1.0\n",
      "Epoch 1/20, Train Loss: 0.7251, Val Loss: 1.4147\n",
      "Epoch 2/20, Train Loss: 0.6347, Val Loss: 1.5333\n",
      "Epoch 3/20, Train Loss: 0.5753, Val Loss: 1.6906\n",
      "Epoch 4/20, Train Loss: 0.5273, Val Loss: 1.8561\n",
      "Epoch 5/20, Train Loss: 0.4867, Val Loss: 1.9765\n",
      "Epoch 6/20, Train Loss: 0.4533, Val Loss: 2.2751\n",
      "Early stopping at epoch 6\n",
      "Multi-class AUC: 0.6697037322289842\n",
      "Epoch 1/20, Train Loss: 0.6343, Val Loss: 0.8018\n",
      "Epoch 2/20, Train Loss: 0.5140, Val Loss: 0.6999\n",
      "Epoch 3/20, Train Loss: 0.4535, Val Loss: 0.6070\n",
      "Epoch 4/20, Train Loss: 0.4145, Val Loss: 0.5918\n",
      "Epoch 5/20, Train Loss: 0.3839, Val Loss: 0.6107\n",
      "Epoch 6/20, Train Loss: 0.3633, Val Loss: 0.5644\n",
      "Epoch 7/20, Train Loss: 0.3474, Val Loss: 0.6315\n",
      "Epoch 8/20, Train Loss: 0.3319, Val Loss: 0.6507\n",
      "Epoch 9/20, Train Loss: 0.3200, Val Loss: 0.7258\n",
      "Epoch 10/20, Train Loss: 0.3119, Val Loss: 0.5969\n",
      "Epoch 11/20, Train Loss: 0.3033, Val Loss: 0.6986\n",
      "Early stopping at epoch 11\n",
      "Multi-class AUC: 0.84549316936441\n"
     ]
    }
   ],
   "source": [
    "# For binary classification (gender)\n",
    "y_train_le_gender = le.fit_transform(y_train['gender'])\n",
    "y_test_le_gender = le.transform(y_test['gender'])\n",
    "binary_model, gender_auc = train_binary_model(X_train_scaled, y_train_le_gender, X_test_scaled, y_test_le_gender,weights=[1600/300])\n",
    "\n",
    "# For binary classification (hold racket handed)\n",
    "y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n",
    "y_test_le_hold = le.transform(y_test['hold racket handed'])\n",
    "hold_model, hold_auc = train_binary_model(X_train_scaled, y_train_le_hold, X_test_scaled, y_test_le_hold,weights=[1600/350])\n",
    "\n",
    "# For multi-class classification (play years)\n",
    "y_train_le_years = le.fit_transform(y_train['play years'])\n",
    "y_test_le_years = le.transform(y_test['play years'])\n",
    "years_model, years_auc = train_multi_model(X_train_scaled, y_train_le_years, X_test_scaled, y_test_le_years,weights=[400,900,700])\n",
    "\n",
    "# For multi-class classification (level)\n",
    "y_train_le_level = le.fit_transform(y_train['level'])\n",
    "y_test_le_level = le.transform(y_test['level'])\n",
    "level_model, level_auc = train_multi_model(X_train_scaled, y_train_le_level, X_test_scaled, y_test_le_level,weights=[700,200,150,900])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_binary(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict_proba(X_test)\n",
    "    print(predicted.shape) \n",
    "    # 取出正類（index 0）的概率\n",
    "    predicted = [predicted[i][0] for i in range(len(predicted))]\n",
    "\n",
    "    \n",
    "    num_groups = len(predicted) // group_size \n",
    "    if sum(predicted[:group_size]) / group_size > 0.5:\n",
    "        y_pred = [max(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "    else:\n",
    "        y_pred = [min(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "    \n",
    "    y_pred  = [1 - x for x in y_pred]\n",
    "    y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test_agg, y_pred, average='micro')\n",
    "    print(auc_score)\n",
    "\n",
    "def model_multiary(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict_proba(X_test)\n",
    "    num_groups = len(predicted) // group_size\n",
    "    y_pred = []\n",
    "    for i in range(num_groups):\n",
    "        group_pred = predicted[i*group_size: (i+1)*group_size]\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        # 對每個類別計算該組內的總機率\n",
    "        class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n",
    "        chosen_class = np.argmax(class_sums)\n",
    "        candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n",
    "        best_instance = np.argmax(candidate_probs)\n",
    "        y_pred.append(group_pred[best_instance])\n",
    "    \n",
    "    y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "    auc_score = roc_auc_score(y_test_agg, y_pred, average='micro', multi_class='ovr')\n",
    "    print('Multiary AUC:', auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10746, 2)\n",
      "0.7907995618838993\n"
     ]
    }
   ],
   "source": [
    "# 評分：針對各目標進行模型訓練與評分\n",
    "y_train_le_gender = le.fit_transform(y_train['gender'])\n",
    "y_test_le_gender = le.transform(y_test['gender'])\n",
    "model_binary(X_train_scaled, y_train_le_gender, X_test_scaled, y_test_le_gender)\n",
    "\n",
    "# y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n",
    "# y_test_le_hold = le.transform(y_test['hold racket handed'])\n",
    "# model_binary(X_train_scaled, y_train_le_hold, X_test_scaled, y_test_le_hold)\n",
    "\n",
    "# y_train_le_years = le.fit_transform(y_train['play years'])\n",
    "# y_test_le_years = le.transform(y_test['play years'])\n",
    "# model_multiary(X_train_scaled, y_train_le_years, X_test_scaled, y_test_le_years)\n",
    "\n",
    "# y_train_le_level = le.fit_transform(y_train['level'])\n",
    "# y_test_le_level = le.transform(y_test['level'])\n",
    "# model_multiary(X_train_scaled, y_train_le_level, X_test_scaled, y_test_le_level)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
